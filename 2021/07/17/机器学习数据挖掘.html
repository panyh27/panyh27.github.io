<h4 id="为什么用交叉验证法"><strong>为什么用交叉验证法？</strong></h4>

<ol>
  <li>交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合。</li>
  <li>还可以从有限的数据中获取尽可能多的有效信息。</li>
</ol>

<h4 id="归一化">归一化</h4>

<p>一是，部分模型的求解需要；二是，提升优化算法收敛速度，这两个作用对于不同模型表现有所不同。</p>

<p>其中，必须进行归一化处理的模型有：支持向量机、K最近邻、神经网络和主成分分析</p>

<h4 id="维度灾难">维度灾难</h4>

<p>在样本量一定的情况下，维度越高，样本在空间中的分布越呈现稀疏性。</p>

<p>这种分布的稀疏性带来2个不好的影响：</p>

<p>1、 模型参数难以正确估计（例如：样本不够时，得出的决策边界往往是过拟合的）。假设均匀分布下，一维空间（即数轴上的某一区间）需要N个样本才能完全覆盖，那么二维均匀分布下就需要N^2个样本才能完全覆盖,三维N^3个,依次类推。因此，随着维度的增加，理论上需要指数增长的样本数量覆盖到整个样本空间上时，才能保证模型能有效的估计参数。而对于那些具有非线性决策边界的分类器（如：神经网络，决策树，KNN）来说，如果样本不够多时，往往很容易过拟合。</p>

<p>2、 样本分布位于中心附近的概率，随着维度的增加，越来越低；而样本处在边缘的概率，则越来越高。通常来讲，样本的特征位于边缘时，比位于中心区域附近更难分类，因为边缘样本的特征取值范围变化太大。想象二维空间下的两个同心圆，假设r1=0.5，r2=1，那么面积之比为1/4；如果半径不变，在三维空间中，体积之比变成1/8；到了8维空间下，超球体的体积之比为1/256，仅仅占到2%。当维数趋于无穷时，位于中心附近的概率趋于0。这种情况下，一些度量相异性的距离指标（如：欧式距离）效果会大大折扣，从而导致一些基于这些指标的分类器在高维度的时候表现不好。</p>

<h4 id="pca">PCA</h4>

<p>输入：数据集 <img src="https://www.zhihu.com/equation?tex=X%3D%5Cleft%5C%7B+x_%7B1%7D%2Cx_%7B2%7D%2Cx_%7B3%7D%2C...%2Cx_%7Bn%7D+%5Cright%5C%7D" alt="[公式]" /> ，需要降到k维。</p>

<p>1) 去平均值(即去中心化)，即每一位特征减去各自的平均值。</p>

<p>2) 计算协方差矩阵 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%7DXX%5ET" alt="[公式]" />,注：这里除或不除样本数量n或n-1,其实对求出的特征向量没有影响。</p>

<p>3) 用特征值分解方法求协方差矩阵<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%7DXX%5ET" alt="[公式]" /> 的特征值与特征向量。</p>

<p>4) 对特征值从大到小排序，选择其中最大的k个。然后将其对应的k个特征向量分别作为行向量组成特征向量矩阵P。</p>

<p>5) 将数据转换到k个特征向量构建的新空间中，即Y=PX。</p>

<h4 id="dbscan理论基本步骤"><strong>DBSCAN理论–基本步骤</strong></h4>

<p><strong>输入：</strong>包含n个对象的集合D，指定半径epislon和最少样本量MinPts。</p>

<p><strong>输出：</strong>所有生成的簇，达到密度要求。</p>

<p>1）repeat</p>

<p>2）从集合D中抽取一个未处理的点；</p>

<p>3）如果抽出的点是核心点，则找出所有从该点出发的密度可达对象，形成簇；</p>

<p>4）如果抽出点的为非核心点，则跳出循环，寻找下一个点；</p>

<p>5）until所有点都被处理。</p>

<h4 id="em">EM</h4>

<p><strong>算法步骤：</strong></p>

<p>（1）随机初始化模型参数θ的初值 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7B0%7D" alt="[公式]" /> 。</p>

<p>（2）j=1,2,…,J 开始EM算法迭代：</p>

<ul>
  <li>E步：计算联合分布的条件概率期望：</li>
</ul>

<p><img src="https://www.zhihu.com/equation?tex=Q_%7Bi%7D%28z_%7Bi%7D%29%3Dp%28z_%7Bi%7D%7Cx_%7Bi%7D%2C%5Ctheta_%7Bj%7D%29" alt="[公式]" /></p>

<p><img src="https://www.zhihu.com/equation?tex=l%28%5Ctheta%2C%5Ctheta_%7Bj%7D%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%5Csum_%7Bz_%7Bi%7D%7D%5E%7B%7D%7BQ_%7Bi%7D%28z_%7Bi%7D%29log%5Cfrac%7Bp%28x_%7Bi%7D%2Cz_%7Bi%7D%3B%5Ctheta%29%7D%7BQ_%7Bi%7D%28z_%7Bi%7D%29%7D%7D%7D" alt="[公式]" /></p>

<ul>
  <li>M步：极大化 <img src="https://www.zhihu.com/equation?tex=l%28%5Ctheta%2C%5Ctheta_%7Bj%7D%29" alt="[公式]" /> ,得到 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj%2B1%7D" alt="[公式]" /> :</li>
</ul>

<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj%2B1%7D%3Dargmaxl%28%5Ctheta%2C%5Ctheta_%7Bj%7D%29" alt="[公式]" /></p>

<ul>
  <li>如果<img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj%2B1%7D" alt="[公式]" /> 已经收敛，则算法结束。否则继续进行E步和M步进行迭代。</li>
</ul>

<p><strong>输出：</strong>模型参数θ。</p>

<p>Q1。深度学习和机器学习的区别是什么?</p>

<p>机器学习涉及从数据模式中学习，然后将其应用于决策的算法。另一方面，深度学习能够通过自身处理数据进行学习，它与人类大脑非常相似，能够识别、分析并做出决定。</p>

<p>主要区别如下:数据呈现给系统的方式。机器学习算法总是需要结构化的数据，而深度学习网络依赖于人工神经网络的层次。</p>

<p>Q2。监督和非监督机器学习的主要区别是什么?</p>

<p>监督学习与非监督学习的主要区别在于:监督学习技术需要有标记的数据来训练模型。例如，要解决一个分类问题(一个有监督的学习任务)，您需要有标签数据来训练模型，并将数据分类到您的标签组中。无监督学习不需要任何标记数据集(聚类算法）。</p>

<p>Q3。在处理数据集时如何选择重要的变量?’</p>

<p>从数据集中选择重要变量的方法有以下几种:</p>

<ol>
  <li>在结束重要变量之前，丢弃相关变量。</li>
  <li>lasso回归。</li>
  <li>随机森林。</li>
  <li>基于信息增益的可用特征集。</li>
</ol>

<p>Q4。目前有很多机器学习算法,如果给定一个数据集，如何确定使用哪个算法?</p>

<p>要使用的机器学习算法完全取决于给定数据集中的数据类型。如果数据是线性的，那么我们使用线性回归。如果数据显示非线性，那么bagging/kernel算法会做得更好。如果数据是为了一些商业目的而进行分析/解释，那么我们可以使用决策树或SVM。如果数据集由图像、视频、音频组成，神经网络将有助于精确求解。</p>

<p>Q5。什么时候正则化开始在机器学习中发挥作用?</p>

<p>当模型开始过拟合时，正则化就变得必要了。它是一种回归，使系数估计数转向或正则化为零。它降低了模型的灵活性，阻碍了模型的学习，以避免过拟合的风险。模型复杂度降低，预测能力提高。</p>

<p>Q6。数据的高方差是好还是坏?</p>

<p>较高的方差直接意味着数据分布较大，具有数据多样性的特征。通常，具有高方差的数据质量不好。</p>

<p>Q7。如果您的数据集存在高方差，您将如何处理它?</p>

<p>对于高方差的数据集，我们可以使用bagging算法来处理它。Bagging算法将数据分割成子组，从随机数据中复制抽样。数据分割后，使用随机数据使用训练算法创建规则。然后利用投票对模型的所有预测结果进行组合。</p>

<p>Q8。随机梯度下降(SGD)和梯度下降(GD)有什么区别?</p>

<p>梯度下降和随机梯度下降算法都能找出使损失函数最小化的一组参数。不同的是，在梯度下降，所有的训练样本评估每一组参数。而在随机梯度下降中，只对识别出的参数集评估一个训练样本</p>

<p>Q9。你能说说决策树的优点和缺点吗?</p>

<p>决策树的优点是它们更容易解释，是非参数的，因此对异常值具有鲁棒性，并且有相对较少的参数需要调整。</p>

<p>另一方面，缺点是它们容易过拟合，忽略属性间的相关性。</p>

<p>Q10。什么是混淆矩阵?你为什么需要它?</p>

<p>混淆矩阵是一个经常用来说明分类模型性能的表。它允许我们可视化算法/模型的性能。它被用作模型/算法的性能度量。用计数值总结正确和错误预测的数量，并按类别分类。它为我们提供了关于通过分类器所犯错误的信息以及分类器所犯错误的类型。</p>

<p>Q11。解释一下“维度诅咒”这个短语。</p>

<p>维数的诅咒指的是数据有太多特征的情况。它涉及的问题如下:如果我们的特征比观察到的多，我们就有过拟合模型的风险。太多的维度会导致数据集中的每个观测结果与其他所有观测结果之间的距离相等（稀疏），没有任何有意义的聚类可以形成。在这种情况下，像PCA这样的降维技术可以帮助解决问题。</p>

<p>Q12。正规化和归一化有什么区别?</p>

<p>归一化校正数据;正则化调整预测函数。</p>

<p>如果您的数据处于非常不同的范围(特别是从低到高)，您可能希望规范化数据，修改每个列以使其兼容基本统计信息，这有助于确保不丢失准确性。</p>

<p>模型训练的目标之一是识别信号而忽略噪声，如果为了使误差最小化而对模型进行任意设置，就有可能出现过拟合。正则化通过提供更简单的拟合函数来控制这一点。</p>

<p>Q13。解释标准化和归一化化之间的区别。</p>

<p>标准化和归一化是两种非常常用的方法。归一化是指重新缩放值以适应[0,1]的范围。标准化是指重新调整数据，使其均值为0，标准差为1。</p>

<p>Q14。区分回归和分类。回归和分类被归类在监督机器学习的同一伞下。它们之间的主要区别是回归输出变量是数值的(或连续的)，而分类输出变量是分类的(或离散的)。例如:预测一个地方的确定温度是回归问题，而预测当天是晴天还是下雨是分类问题。</p>

<p>Q15。哪种机器学习算法被称为懒惰学习者?为什么它被称为懒惰学习者?</p>

<p>KNN是一种被称为懒学习者的机器学习算法。KNN是一种懒惰的学习者，因为它不从训练数据中学习任何机器学习的值或变量，而是在每次要分类时动态计算距离，从而记忆训练数据集。</p>

<p>Q16。K-Means和KNN算法的区别?KNN是监督学习，而K-Means是无监督学习。利用KNN，我们可以根据其最近的邻居预测未知元素的标签，并进一步扩展该方法以解决基于分类/回归的问题。K-Means是无监督学习，在这里我们没有任何标签，换句话说，没有目标变量，因此我们试图基于它们的坐标聚类数据，并试图建立基于为该聚类过滤的元素的聚类的性质</p>

<p>Q17。支持向量机算法的核技巧是什么?核窍门是一个数学函数，当应用于数据点时，可以找到两个不同类别之间的分类区域。基于或函数的选择，无论是线性的还是径向的，这纯粹取决于数据的分布，我们可以建立一个分类器。</p>

<p>Q18。什么是集成模型?与传统的ML分类算法相比，集成技术如何产生更好的学习?</p>

<p>集成是在分类和回归类中一起用于预测的一组模型。集成学习有助于提高ML的结果，因为它结合了多种模式。通过这样做，与单一模型相比，它可以提供更好的预测性能。Hey优于单个模型，因为它们减少了方差，平均了偏差，而且过拟合的机会更小。</p>

<p>Q19。什么是过拟合和欠拟合?为什么决策树算法经常遇到过拟合问题?</p>

<p>过拟合是一种捕捉数据噪声的统计模型或机器学习算法。欠拟合是一种模型或机器学习。当模型或算法显示出低方差但高偏差时，数据拟合不够好。在决策树中，当树被设计为完美地拟合训练数据集中的所有样本时，就会发生过拟合。这将导致具有严格规则或稀疏数据的分支，并影响预测不属于训练集的样本的准确性。</p>

<p>Q20。请解释Lasso和岭回归Ridge之间的区别。</p>

<p>Lasso(L1)和Ridge(L2)是正则化技术，我们通过惩罚系数来找到最优解。在岭，惩罚函数是由系数的平方和定义的。对于Lasso，我们惩罚系数的绝对值之和。另一种类型的正则化方法是ElasticNet弹性网络，它是一个混合惩罚函数，同时具有Lasso和ridge。</p>

<p>Q21。概率和可能性的区别是什么?</p>

<p>概率是事件发生可能性的度量也就是说，特定事件发生的确定性是多少?而似然函数是参数空间内的参数函数，描述获得观测数据的概率。所以根本的区别是，概率与可能的结果有关;可能性依附于假设。</p>

<p>Q22。模型准确性还是模型性能?你更喜欢哪一个?我们应该先弄清楚什么是模型性能?如果性能意味着速度，那么它取决于应用程序的性质，任何与实时场景相关的应用程序都需要将高速作为一个重要特性。如果性能被暗示为为什么准确性不是最重要的优点，对于任何不平衡的数据集，它将是一个F1分数将解释业务案例。</p>

<p>Q23。如何处理不平衡的数据集?采样技术可以帮助处理不平衡的数据集。有两种方法执行抽样，下抽样或过抽样。在Under Sampling中，我们减少了大多数类的大小来匹配少数类，因此有助于提高w.rt存储和运行时执行的性能，但它可能会丢弃有用的信息。对于过采样，我们对Minority类进行上采样，从而解决了信息丢失的问题，但却遇到了过拟合的问题。</p>

<p>Q24。说说为什么特征工程在模型构建中很重要。数据最初是原始形式的。在将数据提供给算法之前，需要从数据中提取特征。这个过程被称为特征工程。当你有相关的特征时，算法的复杂度就会降低。． 然后，即使使用了非理想的算法，结果也是准确的。特性工程主要有两个目标:准备合适的输入数据集，以兼容机器学习算法的约束。提高机器学习模型的性能。</p>

<p>Q25。区别助推和装袋?套袋和助推是集合技术的变种。Bagging是一种用于减少varíance的算法具有很高的。方差。决策树是一类具有高度偏倚的分类器。Boosting是使用一个n-弱分类器系统进行预测，使每个弱分类器都能弥补其分类器的弱点的过程。</p>

<p>Q26。生成性模型和区别性模型的区别是什么?生成模型学习联合概率分布p(x,y)。利用贝叶斯定理对条件概率进行预测。判别模型学习条件概率分布p(ylx)。这两种模型通常用于监督学习问题。一般来说，一个判别模型模型的决策边界之间的类。生成模型明确地模拟了每个类的实际分布。</p>

<p>Q27。什么是超参数，它们与参数有什么不同?参数是模型内部的一个变量，它的值从训练数据中估计出来。它们通常被保存为学习模型的一部分。例如权重等超参数是模型外部的一个变量，它的值不能从数据中估计。它们经常被用来估计模型参数。超参数的选择对实现是敏感的。例子包括学习率，隐藏层等。</p>

<p>Q28。如何在聚类算法中定义聚类的数量?集群的数量可以通过寻找剪影得分来确定。通常，我们的目标是使用聚类技术从数据中得到一些推论，这样我们就可以对数据所表示的许多类有一个更广泛的了解。在这种情况下，剪影得分帮助我们确定聚类中心的数量，以便沿着这些聚类我们的数据。</p>

<p>Q29。交叉验证的作用是什么?交叉验证是一种用于提高机器学习算法性能的技术，在这种技术中，机器将从相同的数据中多次输入采样数据。采样完成后，数据集被分成等量行数的小部分，随机部分被选为测试集，而所有其他部分被选为训练集。</p>

<p>Q30。KNN中可以使用什么距离度量?在KNN中可以使用跟踪距离度量。曼哈顿闵可夫斯基Jaccard余弦</p>

<p>Q31。随机森林使用了什么集合技术?套袋是《Random Forests》所使用的技术。随机森林是树木的集合，它对原始数据集的采样数据进行工作，最终的预测是所有树木的投票平均值。</p>

<p>Q32。熵和信息增益的区别是什么?信息增益是基于对数据集进行属性分割后熵的减小。构建决策树的关键在于找到返回最高信息增益的属性(即，最同构的分支)。</p>

<p>Q33。给出一些你将使用SVM而不是随机森林的情况。改用支持向量机的主要原因是问题可能不是线性可分的。在这种情况下，我们将不得不使用具有非线性核的支持向量机(例如RBF)。． 使用支持向量机的另一个相关原因是如果您处于高维空间中。例如，据报道，支持向量机可以更好地用于文本分类。.</p>

<p>Q34。支持向量机作为一个大边际分类器，它是否受到极端值的影响?</p>

<p>说明:是的，如果C是大的，否则不是。</p>

<p>Q35。我们能把核技巧应用到逻辑回归中吗?</p>

<p>逻辑回归在计算上比支持向量机-O(N)比O(N2k)更昂贵，其中k是支持向量的数量。．SVM中的分类器被设计成仅根据支持向量定义，而在Logistic回归中，分类器是在所有点上定义的，而不仅仅是支持向量。这使得支持向量机可以享受一些自然的加速(在高效代码编写方面)，而这是逻辑回归难以实现的</p>

<p>Q36。支持向量机是否给出任何概率输出?支持向量机不直接提供概率估计，这些是通过昂贵的五倍交叉验证来计算的</p>

<p>Q37。告诉贝叶斯风险和经验风险之间的区别..贝叶斯决策函数f:X-cA是在所有可能函数中实现最小风险的函数。一个贝叶斯决策函数的风险称为贝叶斯风险。贝叶斯决策函数通常被称为“目标函数”，因为它是我们可能产生的最佳决策函数。f:X-cA相对于Dn的经验风险为* ()e (f (x, y)。</p>

<p>Q38。什么是拉格朗日乘数?在数学优化中，拉格朗日乘子法是一种寻找受等式约束的函数的局部极大值和最小值的策略(即，一个或多个方程必须由所选变量的值所满足的条件)。其基本思想是将一个有约束的问题转换成一种形式，使无约束问题的导数检验仍然可以应用。拉格朗日对偶函数给出了最优解的下界。</p>

<p>Q39。随机森林的随机性在哪里?在构建随机森林中的每棵树时，为了保证每棵树的独立性，我们通常采用两级随机。1. 随机选取数据(采用套袋法)进行替换。2. 随机选择m个特征。</p>

<p>Q41。说明深度学习模型和集成模型之间的相关性深度学习模型如CNN和变形金刚广泛采用集成策略。</p>

<p>Q42。分类器的集合可能不会比它的任何单个模型更精确。真或假?为什么?真实的。通常，集成会改进模型，但这不是必要的。</p>
